{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f5aa5e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b4fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from functools import reduce\n",
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb7501",
   "metadata": {},
   "source": [
    "# Fetch 4 neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4c80aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GEOID20', 'CA', 'COMMUNIT_1'], dtype='object')\n",
      "       GEOID20  CA   COMMUNIT_1\n",
      "0  17031010100   1  Rogers Park\n",
      "1  17031010201   1  Rogers Park\n",
      "2  17031010202   1  Rogers Park\n",
      "3  17031010300   1  Rogers Park\n",
      "4  17031010400   1  Rogers Park\n"
     ]
    }
   ],
   "source": [
    "equiv = pd.read_csv(\"2020 Census Tracts to Chicago Community Area Equivalency File - Sheet1.csv\")\n",
    "\n",
    "\n",
    "print(equiv.columns)\n",
    "print(equiv.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d977414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lakeview': [17031060100, 17031060200, 17031060300, 17031060400, 17031060500, 17031060800, 17031060900, 17031061000, 17031061100, 17031061200, 17031061500, 17031061800, 17031061901, 17031061902, 17031062000, 17031062100, 17031062200, 17031062300, 17031062400, 17031062500, 17031062600, 17031062700, 17031062800, 17031062900, 17031063000, 17031063100, 17031063200, 17031063301, 17031063302, 17031063303, 17031063400, 17031831900, 17031832000, 17031832100], 'LincolnPark': [17031070101, 17031070102, 17031070103, 17031070200, 17031070300, 17031070400, 17031070500, 17031070600, 17031070700, 17031071000, 17031071100, 17031071200, 17031071300, 17031071400, 17031071500, 17031071600, 17031071700, 17031071800, 17031832500, 17031832600], 'NearNorthSide': [17031080100, 17031080201, 17031080202, 17031080300, 17031080400, 17031081000, 17031081100, 17031081201, 17031081202, 17031081300, 17031081401, 17031081402, 17031081403, 17031081500, 17031081600, 17031081700, 17031081800, 17031081900, 17031838300, 17031842200, 17031842300], 'JeffersonPark': [17031110100, 17031110200, 17031110300, 17031110400, 17031110501, 17031110502]}\n"
     ]
    }
   ],
   "source": [
    "attributes = {\n",
    "    \"unemployment\": \"B23025_005E\",\n",
    "    \"graduate\": \"B15003_023E\",\n",
    "    \"bachelors\": \"B15003_022E\",\n",
    "    \"snap\": \"B22003_002E\",\n",
    "    \"median_income\": \"B19013_001E\",\n",
    "    \"poverty\": \"B17001_002E\",\n",
    "    \"broadband\": \"B28002_004E\",\n",
    "    \"owner_occupied\": \"B25003_002E\",\n",
    "    \"renter_occupied\": \"B25003_003E\",\n",
    "    \"total_population\": \"B03002_001E\",\n",
    "    \"white\": \"B03002_003E\",\n",
    "    \"black\": \"B03002_004E\",\n",
    "    \"hispanic\": \"B03002_012E\"\n",
    "}\n",
    "\n",
    "tract_lists = {\n",
    "    \"Lakeview\": equiv[equiv[\"COMMUNIT_1\"] == \"Lake View\"][\"GEOID20\"].tolist(),\n",
    "    \"LincolnPark\": equiv[equiv[\"COMMUNIT_1\"] == \"Lincoln Park\"][\"GEOID20\"].tolist(),\n",
    "    \"NearNorthSide\": equiv[equiv[\"COMMUNIT_1\"] == \"Near North Side\"][\"GEOID20\"].tolist(),\n",
    "    \"JeffersonPark\": equiv[equiv[\"COMMUNIT_1\"] == \"Jefferson Park\"][\"GEOID20\"].tolist()\n",
    "}\n",
    "\n",
    "print(tract_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6d75ec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m fetch_acs(year, [code])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Filter to neighborhood tracts\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin([tid[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m tid \u001b[38;5;129;01min\u001b[39;00m tract_ids])]\n\u001b[1;32m     22\u001b[0m df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, code, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounty\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtract\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYEAR\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     23\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "Cell \u001b[0;32mIn[29], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m fetch_acs(year, [code])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Filter to neighborhood tracts\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin([tid[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m tid \u001b[38;5;129;01min\u001b[39;00m tract_ids])]\n\u001b[1;32m     22\u001b[0m df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, code, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounty\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtract\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYEAR\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     23\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Now your loop works without slicing again\n",
    "for hood, tract_ids in tract_lists.items():\n",
    "    os.makedirs(f\"ONSA_Data/{hood}\", exist_ok=True)\n",
    "\n",
    "    for attr, code in attributes.items():\n",
    "        dfs = []\n",
    "        for year in range(2018, 2024):\n",
    "            df = fetch_acs(year, [code])\n",
    "            # tract column is already last 6 digits, so just check membership\n",
    "            df = df[df[\"tract\"].isin(tract_ids)]\n",
    "            df = df[[\"NAME\", code, \"state\", \"county\", \"tract\", \"YEAR\"]]\n",
    "            dfs.append(df)\n",
    "\n",
    "        df_all = pd.concat(dfs, ignore_index=True)\n",
    "        out_path = f\"ONSA_Data/{hood}/{hood}_{attr}_2018_2023.csv\"\n",
    "        df_all.to_csv(out_path, index=False)\n",
    "        print(f\"Saved {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18e8ce",
   "metadata": {},
   "source": [
    "# Calculating geoid + race percentages and majority race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "726a605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [geo_id, YEAR, white_pct, black_pct, hispanic_pct, majority_race]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def build_geo_id(row):\n",
    "    state = str(row['state']).zfill(2)\n",
    "    county = str(row['county']).zfill(3)\n",
    "    tract = str(row['tract']).zfill(6)\n",
    "    return f\"1400000US{state}{county}{tract}\"\n",
    "\n",
    "def fetch_race_from_csvs(neighborhood):\n",
    "    # load pop\n",
    "    total_df = pd.read_csv(f\"ONSA_Data/{neighborhood}/{neighborhood}_total_population_2018_2023.csv\")\n",
    "    total_df['geo_id'] = total_df.apply(build_geo_id, axis=1)\n",
    "\n",
    "    # load race \n",
    "    white_df = pd.read_csv(f\"ONSA_Data/{neighborhood}/{neighborhood}_white_2018_2023.csv\")\n",
    "    black_df = pd.read_csv(f\"ONSA_Data/{neighborhood}/{neighborhood}_black_2018_2023.csv\")\n",
    "    hisp_df  = pd.read_csv(f\"ONSA_Data/{neighborhood}/{neighborhood}_hispanic_2018_2023.csv\")\n",
    "\n",
    "    for df in [white_df, black_df, hisp_df]:\n",
    "        df['geo_id'] = df.apply(build_geo_id, axis=1)\n",
    "\n",
    "    merged = total_df.merge(white_df, on=[\"geo_id\",\"YEAR\"], suffixes=(\"\", \"_white\"))\n",
    "    merged = merged.merge(black_df, on=[\"geo_id\",\"YEAR\"], suffixes=(\"\", \"_black\"))\n",
    "    merged = merged.merge(hisp_df,  on=[\"geo_id\",\"YEAR\"], suffixes=(\"\", \"_hisp\"))\n",
    "\n",
    "    # extract cols\n",
    "    total_col = \"B03002_001E\"\n",
    "    white_col = \"B03002_003E\"\n",
    "    black_col = \"B03002_004E\"\n",
    "    hisp_col  = \"B03002_012E\"\n",
    "\n",
    "    for col in [total_col, white_col, black_col, hisp_col]:\n",
    "        merged[col] = pd.to_numeric(merged[col], errors=\"coerce\")\n",
    "\n",
    "    # compute %\n",
    "    merged[\"white_pct\"]    = merged[white_col] / merged[total_col] * 100\n",
    "    merged[\"black_pct\"]    = merged[black_col] / merged[total_col] * 100\n",
    "    merged[\"hispanic_pct\"] = merged[hisp_col]  / merged[total_col] * 100\n",
    "\n",
    "    # majority race\n",
    "    def majority(row):\n",
    "        races = {\n",
    "            \"white\": row[\"white_pct\"],\n",
    "            \"black\": row[\"black_pct\"],\n",
    "            \"hispanic\": row[\"hispanic_pct\"]\n",
    "        }\n",
    "        return max(races, key=races.get)\n",
    "\n",
    "    merged[\"majority_race\"] = merged.apply(majority, axis=1)\n",
    "\n",
    "    return merged[[\"geo_id\",\"YEAR\",\"white_pct\",\"black_pct\",\"hispanic_pct\",\"majority_race\"]]\n",
    "\n",
    "\n",
    "df_race = fetch_race_from_csvs(\"LincolnPark\")\n",
    "print(df_race.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32323a17",
   "metadata": {},
   "source": [
    "# aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f75a8eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'white_count_2018'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'white_count_2018'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m hisp_c  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhispanic_count_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m total_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_population_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 94\u001b[0m race_merged[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite_pct_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]    \u001b[38;5;241m=\u001b[39m race_merged[white_c] \u001b[38;5;241m/\u001b[39m race_merged[total_c] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     95\u001b[0m race_merged[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack_pct_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]    \u001b[38;5;241m=\u001b[39m race_merged[black_c] \u001b[38;5;241m/\u001b[39m race_merged[total_c] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     96\u001b[0m race_merged[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhispanic_pct_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m race_merged[hisp_c]  \u001b[38;5;241m/\u001b[39m race_merged[total_c] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'white_count_2018'"
     ]
    }
   ],
   "source": [
    "neighborhoods = [\n",
    "    \"Irving_Park\",\n",
    "    \"Englewood\",\n",
    "    \"Portage_Park\",\n",
    "    \"South_Lawndale\",\n",
    "    \"West_Englewood\",\n",
    "    \"LincolnPark\",\n",
    "    \"NearNorthSide\",\n",
    "    \"JeffersonPark\",\n",
    "    \"Lakeview\"\n",
    "    \n",
    "]\n",
    "\n",
    "attributes = {\n",
    "    \"unemployment\": \"B23025_005E\",\n",
    "    \"graduate\": \"B15003_023E\",\n",
    "    \"bachelors\": \"B15003_022E\",\n",
    "    \"snap\": \"B22003_002E\",\n",
    "    \"median_income\": \"B19013_001E\",\n",
    "    \"poverty\": \"B17001_002E\",\n",
    "    \"broadband\": \"B28002_004E\",\n",
    "    \"owner_occupied\": \"B25003_002E\",\n",
    "    \"renter_occupied\": \"B25003_003E\",\n",
    "    \"unemployment\": \"B23025_005E\",\n",
    "    \"total_population\": \"B03002_001E\"\n",
    "}\n",
    "\n",
    "\n",
    "combined_neighborhoods = []\n",
    "\n",
    "for hood in neighborhoods:\n",
    "    dfs = []\n",
    "\n",
    "    \n",
    "    for attr, col_code in attributes.items():\n",
    "        file_path = f\"ONSA_Data/{hood}/{hood}_{attr}_2018_2023.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Missing file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        df['geo_id'] = df.apply(build_geo_id, axis=1)\n",
    "        value_col = col_code if col_code in df.columns else df.columns[1]\n",
    "        df_wide = df.pivot(index=\"geo_id\", columns=\"YEAR\", values=value_col)\n",
    "        df_wide.columns = [f\"{attr}_{year}\" for year in df_wide.columns]\n",
    "        df_wide.reset_index(inplace=True)\n",
    "\n",
    "        dfs.append(df_wide)\n",
    "\n",
    "    # === Load race counts from CSVs, compute percentages + majority race\n",
    "    race_dfs = {}\n",
    "    for race in ['white', 'black', 'hispanic']:\n",
    "        race_path = f\"ONSA_Data/{hood}/{hood}_{race}_2018_2023.csv\"\n",
    "        try:\n",
    "            race_df = pd.read_csv(race_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Missing race file: {race_path}\")\n",
    "            continue\n",
    "\n",
    "        race_df['geo_id'] = race_df.apply(build_geo_id, axis=1)\n",
    "        # The saved race CSVs hold counts in the 2nd column (e.g., B03002_003E)\n",
    "        count_col = race_df.columns[1]\n",
    "        race_wide = race_df.pivot(index=\"geo_id\", columns=\"YEAR\", values=count_col)\n",
    "        race_wide.columns = [f\"{race}_count_{year}\" for year in race_wide.columns]\n",
    "        race_wide.reset_index(inplace=True)\n",
    "        race_dfs[race] = race_wide\n",
    "\n",
    "    if race_dfs:\n",
    "        race_counts = reduce(lambda left, right: pd.merge(left, right, on=\"geo_id\", how=\"outer\"), race_dfs.values())\n",
    "\n",
    "        # Load total population wide\n",
    "        total_pop_path = f\"ONSA_Data/{hood}/{hood}_total_population_2018_2023.csv\"\n",
    "        try:\n",
    "            total_pop_df = pd.read_csv(total_pop_path)\n",
    "            total_pop_df['geo_id'] = total_pop_df.apply(build_geo_id, axis=1)\n",
    "            total_col = \"B03002_001E\" if \"B03002_001E\" in total_pop_df.columns else total_pop_df.columns[1]\n",
    "            total_pop_wide = total_pop_df.pivot(index=\"geo_id\", columns=\"YEAR\", values=total_col)\n",
    "            total_pop_wide.columns = [f\"total_population_{year}\" for year in total_pop_wide.columns]\n",
    "            total_pop_wide.reset_index(inplace=True)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Missing total population file: {total_pop_path}\")\n",
    "            total_pop_wide = pd.DataFrame(columns=[\"geo_id\"])\n",
    "\n",
    "        race_merged = pd.merge(race_counts, total_pop_wide, on=\"geo_id\", how=\"outer\")\n",
    "\n",
    "        # Compute percentages and majority race per year\n",
    "        for year in range(2018, 2024):\n",
    "            white_c = f\"white_count_{year}\"\n",
    "            black_c = f\"black_count_{year}\"\n",
    "            hisp_c  = f\"hispanic_count_{year}\"\n",
    "            total_c = f\"total_population_{year}\"\n",
    "\n",
    "            race_merged[f\"white_pct_{year}\"]    = race_merged[white_c] / race_merged[total_c] * 100\n",
    "            race_merged[f\"black_pct_{year}\"]    = race_merged[black_c] / race_merged[total_c] * 100\n",
    "            race_merged[f\"hispanic_pct_{year}\"] = race_merged[hisp_c]  / race_merged[total_c] * 100\n",
    "\n",
    "            def majority(row):\n",
    "                vals = {\n",
    "                    \"white\":    row.get(f\"white_pct_{year}\"),\n",
    "                    \"black\":    row.get(f\"black_pct_{year}\"),\n",
    "                    \"hispanic\": row.get(f\"hispanic_pct_{year}\")\n",
    "                }\n",
    "                vals = {k: (-1 if pd.isna(v) else v) for k, v in vals.items()}\n",
    "                return max(vals, key=vals.get)\n",
    "\n",
    "            race_merged[f\"majority_race_{year}\"] = race_merged.apply(majority, axis=1)\n",
    "\n",
    "        # Keep only pct and majority columns (plus geo_id)\n",
    "        keep_cols = [\"geo_id\"] + [c for c in race_merged.columns if c.startswith((\"white_pct_\", \"black_pct_\", \"hispanic_pct_\", \"majority_race_\"))]\n",
    "        race_merged = race_merged[keep_cols]\n",
    "\n",
    "        dfs.append(race_merged)  # only append this race DF\n",
    "\n",
    "\n",
    "    # merge all\n",
    "    if dfs:\n",
    "        merged = reduce(lambda left, right: pd.merge(left, right, on=\"geo_id\", how=\"outer\"), dfs)\n",
    "        merged['neighborhood'] = hood\n",
    "        combined_neighborhoods.append(merged)\n",
    "\n",
    "\n",
    "final_df = pd.concat(combined_neighborhoods, ignore_index=True)\n",
    "final_df = final_df.loc[:, ~final_df.columns.str.endswith('_y')]\n",
    "final_df.columns = final_df.columns.str.replace('_x', '', regex=False)\n",
    "\n",
    "\n",
    "print(final_df.head())\n",
    "#final_df.to_csv(\"agg_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3cfac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
